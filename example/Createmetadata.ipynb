{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f11d7211",
   "metadata": {},
   "source": [
    "# Algorithm for datasets aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1f275",
   "metadata": {},
   "source": [
    "## Auxiliar function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf7acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Auxilar function to open files\n",
    "def open_df(file_path):\n",
    "    if \".sav\" in file_path:\n",
    "        df = pd.read_spss(file_path,convert_categoricals=False)\n",
    "    elif \".xlsx\" in file_path:\n",
    "        df = pd.read_excel(file_path,engine=\"openpyxl\")\n",
    "    elif \".csv\" in file_path:\n",
    "        df = pd.read_csv(file_path,sep=\",\")\n",
    "    print(file_path)\n",
    "    df.dropna(how='all',inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6caf05",
   "metadata": {},
   "source": [
    "## Read folder \\& and construct metadata dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e11be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset3.csv\n",
      "-> socialdemographics\n",
      "-> variable3\n",
      "-> variable2\n",
      "-> variable1\n",
      "dataset2.csv\n",
      "-> socialdemographics\n",
      "-> variable3\n",
      "-> variable2\n",
      "-> variable1\n",
      "dataset1.csv\n",
      "-> socialdemographics\n",
      "-> variable3\n",
      "-> variable2\n",
      "-> variable1\n",
      "dataset3.csv socialdemographics {'age': 'Alter', 'sex': 'Geschlecht', 'diagnostic': 'Diagnose'}\n",
      "dataset3.csv variable3 {'textfeld1': ('pre_feld1_v1', 'post_feld1_v2'), 'textfeld2': ('pre_feld2_v1', 'post_feld2_v2'), 'textfeld3': ('pre_feld3_v1', 'post_feld3_v2'), 'textfeld4': ('pre_feld4_v1', 'post_feld4_v2'), 'textfeld5': ('pre_feld5_v1', 'post_feld5_v2'), 'textfeld6': ('pre_feld6_v1', 'post_feld6_v2'), 'textfeld7': ('pre_feld7_v1', 'post_feld7_v2'), 'textfeld8': ('pre_feld8_v1', 'post_feld8_v2'), 'textfeld9': ('pre_feld9_v1', 'post_feld9_v2'), 'textfeld_total': ('', '')}\n",
      "dataset3.csv variable2 {'campo_1': ('x_1', 'y_1'), 'campo_2': ('x_2', 'y_2'), 'campo_3': ('x_3', 'y_3'), 'campo_4': ('x_4', 'y_4'), 'campo_total': ('', '')}\n",
      "dataset3.csv variable1 {'field1': ('E_01', 'F_01'), 'field2': ('E_02', 'F_02'), 'field3': ('E_03', 'F_03'), 'field4': ('E_04', 'F_04'), 'field5': ('E_05', 'F_05'), 'field6': ('E_06', 'F_06'), 'field_T': ('', '')}\n",
      "dataset2.csv socialdemographics {'age': 'Age', 'sex': 'Sex', 'diagnostic': 'Diagnostic'}\n",
      "dataset2.csv variable3 {'textfeld1': ('pre_feld1', 'post_feld1'), 'textfeld2': ('pre_feld2', 'post_feld2'), 'textfeld3': ('pre_feld3', 'post_feld3'), 'textfeld4': ('pre_feld4', 'post_feld4'), 'textfeld5': ('pre_feld5', 'post_feld5'), 'textfeld6': ('pre_feld6', 'post_feld6'), 'textfeld7': ('pre_feld7', 'post_feld7'), 'textfeld8': ('pre_feld8', 'post_feld8'), 'textfeld9': ('pre_feld9', 'post_feld9'), 'textfeld_total': ('', '')}\n",
      "dataset2.csv variable2 {'campo_1': ('bb_1', 'dd_1'), 'campo_2': ('bb_2', 'dd_2'), 'campo_3': ('bb_3', 'dd_3'), 'campo_4': ('bb_4', 'dd_4'), 'campo_total': ('', '')}\n",
      "dataset2.csv variable1 {'field1': ('C_01', 'D_01'), 'field2': ('C_02', 'D_02'), 'field3': ('C_03', 'D_03'), 'field4': ('C_04', 'D_04'), 'field5': ('C_05', 'D_05'), 'field6': ('C_06', 'D_06'), 'field_T': ('', '')}\n",
      "dataset1.csv socialdemographics {'age': 'Edad', 'sex': 'Genero', 'diagnostic': 'Diagnostico'}\n",
      "dataset1.csv variable3 {'textfeld1': ('pre_textfeld1', 'post_textfeld1'), 'textfeld2': ('pre_textfeld2', 'post_textfeld2'), 'textfeld3': ('pre_textfeld3', 'post_textfeld3'), 'textfeld4': ('pre_textfeld4', 'post_textfeld4'), 'textfeld5': ('pre_textfeld5', 'post_textfeld5'), 'textfeld6': ('pre_textfeld6', 'post_textfeld6'), 'textfeld7': ('pre_textfeld7', 'post_textfeld7'), 'textfeld8': ('pre_textfeld8', 'post_textfeld8'), 'textfeld9': ('pre_textfeld9', 'post_textfeld9'), 'textfeld_total': ('', '')}\n",
      "dataset1.csv variable2 {'campo_1': ('c_1', 'd_1'), 'campo_2': ('c_2', 'd_2'), 'campo_3': ('c_3', 'd_3'), 'campo_4': ('c_4', 'd_4'), 'campo_total': ('', '')}\n",
      "dataset1.csv variable1 {'field1': ('A_01', 'B_01'), 'field2': ('A_02', 'B_02'), 'field3': ('A_03', 'B_03'), 'field4': ('A_04', 'B_04'), 'field5': ('A_05', 'B_05'), 'field6': ('A_06', 'B_06'), 'field_T': ('', '')}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# create dictionary meta-data from aggregation folder\n",
    "\n",
    "def clean_spaces(l):\n",
    "    return [w.strip() for w in l]\n",
    "\n",
    "# Todo: read datasets names from input_file folder\n",
    "\n",
    "# read folder aggregation\n",
    "metadata = {}\n",
    "for dataset in os.listdir('databases'):\n",
    "    if os.path.isdir(os.path.join('aggregation',dataset)):\n",
    "        metadata[dataset] = {}\n",
    "        print(dataset)\n",
    "        for constructo in os.listdir(os.path.join('aggregation',dataset)):\n",
    "            if constructo.endswith('.csv'):\n",
    "                constructo = constructo.replace('.csv','')\n",
    "                metadata[dataset][constructo] = {}\n",
    "                \n",
    "                df = pd.read_csv(os.path.join('aggregation',dataset,constructo+\".csv\"),sep=',')\n",
    "                df.to_excel(os.path.join('aggregation',dataset,constructo+\".xlsx\"),index=False)\n",
    "\n",
    "                # In linux\n",
    "                with open(os.path.join('aggregation',dataset,constructo+\".csv\"),'r',encoding='cp1252') as f:\n",
    "                # In windows\n",
    "                #with open(os.path.join('aggregation',dataset,constructo+\".csv\"),'r',encoding='ansi') as f:\n",
    "                    lines = f.readlines()\n",
    "                    print(\"->\",constructo)\n",
    "                    if constructo == 'socialdemographics':\n",
    "                        keys = clean_spaces(lines[0].replace('\\n','').split(','))\n",
    "                        values = clean_spaces(lines[1].replace('\\n','').split(','))\n",
    "                        metadata[dataset][constructo] = dict(zip(keys,values))\n",
    "                    else:\n",
    "                        keys = clean_spaces(lines[0].replace('\\n','').split(','))\n",
    "                        esp = clean_spaces(lines[1].replace('\\n','').split(','))\n",
    "                        pos = clean_spaces(lines[2].replace('\\n','').split(','))\n",
    "                        metadata[dataset][constructo] = dict(zip(keys[1:],zip(esp[1:],pos[1:])))\n",
    "                        \n",
    "for dataset in metadata:\n",
    "    for constructo in metadata[dataset]:\n",
    "        print(dataset,constructo,metadata[dataset][constructo])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b2ec4",
   "metadata": {},
   "source": [
    "## Build the algorithms dictionary with columns names to matching between each dataset and the aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b4e30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "databases/dataset3.csv\n",
      "databases/dataset2.csv\n",
      "databases/dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "def insert_dataset(datset,ma,mar,k,v,i):\n",
    "    \n",
    "    ma[dataset][k+'_'+str(i)] = v\n",
    "    if v!='666' and v !='':\n",
    "        if v not in mar[dataset]:\n",
    "            mar[dataset][v] = k+'_'+str(i)\n",
    "        else:\n",
    "            count = sum(key.startswith(k+'_') for key in mar[dataset].keys())\n",
    "            mar[dataset][v+'_'+str(count)] = k+'_'+str(i)\n",
    "                \n",
    "\n",
    "df_datasets = {}\n",
    "map_atributes = {}\n",
    "map_atributes_rev = {}\n",
    "for dataset in metadata:\n",
    "    \n",
    "    map_atributes[dataset] = {}\n",
    "    map_atributes_rev[dataset] = {}\n",
    "    \n",
    "    df_datasets[dataset]= open_df(os.path.join('databases',dataset))\n",
    "\n",
    "    for constructo in metadata[dataset]:\n",
    "        if constructo == 'socialdemographics':\n",
    "            for key,value in metadata[dataset][constructo].items():\n",
    "                #if key == 'quototal':\n",
    "                #     print(dataset,constructo,key,value)\n",
    "                #print(dataset,key,value)\n",
    "                values = value.split(';')\n",
    "                for indx,v in enumerate(values):\n",
    "                        \n",
    "                    #map_atributes[dataset][key+'_'+str(indx)] = v\n",
    "                    #if v!='666' and v !='':\n",
    "                    #    map_atributes_rev[dataset][v] = key+'_'+str(indx)\n",
    "                             \n",
    "                    insert_dataset(dataset,map_atributes,map_atributes_rev,key,v,indx)\n",
    "        else:\n",
    "            for key,value in metadata[dataset][constructo].items():\n",
    "\n",
    "                '''if key == 'qoltotal':\n",
    "                     print(dataset,constructo,key,value)'''\n",
    "                \n",
    "                values = value[0].split(';')\n",
    "                for indx,v in enumerate(values):\n",
    "\n",
    "                        #map_atributes[dataset][\"pre_\"+key+'_'+str(indx)] = v\n",
    "                        #if v!='666' and v !='':\n",
    "                        #    map_atributes_rev[dataset][v] = \"pre_\"+key+'_'+str(indx)\n",
    "                        insert_dataset(dataset,map_atributes,map_atributes_rev,\"pre_\"+key,v,indx)\n",
    "                \n",
    "                values = value[1].split(';')\n",
    "                for indx,v in enumerate(values):\n",
    "                    \n",
    "                           \n",
    "                        insert_dataset(dataset,map_atributes,map_atributes_rev,\"post_\"+key,v,indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf51bdd",
   "metadata": {},
   "source": [
    "## Auxiliar functions keeping a logical order in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef1e1442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database\n",
      "num\n",
      "mct\n",
      "age\n",
      "sex\n",
      "diagnostic\n",
      "field1\n",
      "field2\n",
      "field3\n",
      "field4\n",
      "field5\n",
      "field6\n",
      "field_T\n",
      "campo_1\n",
      "campo_2\n",
      "campo_3\n",
      "campo_4\n",
      "campo_total\n",
      "textfeld1\n",
      "textfeld2\n",
      "textfeld3\n",
      "textfeld4\n",
      "textfeld5\n",
      "textfeld6\n",
      "textfeld7\n",
      "textfeld8\n",
      "textfeld9\n",
      "textfeld_total\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Determine order of columns\n",
    "base_folder = 'variables/'\n",
    "\n",
    "# Impose order of list of variables\n",
    "#variables_order = [\"socialdemographics\",\"Bcis\",\"Bdi\",\"Calgary\",\"Cbq\",\"Cdss\",\"Completion\",\"D15\",\"Dass\",\"Depression\",\"Eq5d\",\"Gaf\",\"Ipsaq\",\"Jtc\",\"Panns\",\"Pbiq\",\"PDD\",\"Pdi\",\"Psyrats\",\"Q-les-q-18\",\"QoLS\",\"Rses\",\"Scip\",\"SECL\",\"Slds\",\"Tmt\",\"Whoqol\"]\n",
    "variables_order = [\"socialdemographics\",\"variable1\",\"variable2\",\"variable3\"]\n",
    "\n",
    "count_variables = [2]\n",
    "variables = ['database','num','mct']\n",
    "for var in variables_order:\n",
    "    with open(base_folder+var+\".csv\",\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if lines != []:\n",
    "            aux = lines[0].strip().split(',')\n",
    "            count_variables.append(len(aux))\n",
    "            variables.extend(aux)\n",
    "\n",
    "for var in variables:\n",
    "    print(var)\n",
    "\n",
    "\n",
    "def sorting_key(c):\n",
    "\n",
    "    pre = 0\n",
    "    if c.startswith('pre_') or c.startswith('post_'):\n",
    "        \n",
    "        if c.startswith('pre_'):\n",
    "            pre = len(variables)\n",
    "        else:\n",
    "            pre = 2*len(variables)\n",
    "\n",
    "        first = c.find(\"_\")\n",
    "        last = c[::-1].find(\"_\")\n",
    "        base = c[first+1:-last-1]\n",
    "        dx = int(c[-last:])\n",
    "        \n",
    "    else:\n",
    "        tokens = c.strip().split('_')\n",
    "        base = '_'.join(tokens[:-1])\n",
    "        dx = int(tokens[-1])  \n",
    "\n",
    "    return [pre,variables.index(base),dx]\n",
    "\n",
    "\n",
    "# Building dictionary MCT\n",
    "mct_dict = {}\n",
    "\n",
    "#base_folder = 'D:\\\\MyDisk\\\\Working-PERMEPSY\\\\databases\\\\'\n",
    "base_folder = '../databases'\n",
    "\n",
    "\n",
    "#with open(os.path.join(base_folder,'input_db','mct_patients.csv'),\"r\") as f:\n",
    "#    lines = f.readlines()\n",
    "#    for line in lines[1:]:\n",
    "#        dataset,_,condition = line.strip().split(';')\n",
    "#        mct_dict[dataset] = condition\n",
    "\n",
    "for dataset in mct_dict:\n",
    "    print(dataset,mct_dict[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a521c80",
   "metadata": {},
   "source": [
    "## Aggregating all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e70cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 aggregating dataset3.csv\n",
      "databases/dataset3.csv\n",
      "\tTotal pacients 53 in dataset3.csv\n",
      "\tTotal net 53 in dataset3.csv\n",
      "1 aggregating dataset2.csv\n",
      "databases/dataset2.csv\n",
      "\tTotal pacients 51 in dataset2.csv\n",
      "\tTotal net 51 in dataset2.csv\n",
      "2 aggregating dataset1.csv\n",
      "databases/dataset1.csv\n",
      "\tTotal pacients 39 in dataset1.csv\n",
      "\tTotal net 39 in dataset1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_0</th>\n",
       "      <th>num_0</th>\n",
       "      <th>age_0</th>\n",
       "      <th>sex_0</th>\n",
       "      <th>diagnostic_0</th>\n",
       "      <th>pre_field1_0</th>\n",
       "      <th>pre_field2_0</th>\n",
       "      <th>pre_field3_0</th>\n",
       "      <th>pre_field4_0</th>\n",
       "      <th>pre_field5_0</th>\n",
       "      <th>...</th>\n",
       "      <th>post_textfeld1_0</th>\n",
       "      <th>post_textfeld2_0</th>\n",
       "      <th>post_textfeld3_0</th>\n",
       "      <th>post_textfeld4_0</th>\n",
       "      <th>post_textfeld5_0</th>\n",
       "      <th>post_textfeld6_0</th>\n",
       "      <th>post_textfeld7_0</th>\n",
       "      <th>post_textfeld8_0</th>\n",
       "      <th>post_textfeld9_0</th>\n",
       "      <th>post_textfeld_total_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset1.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.818327</td>\n",
       "      <td>0.646789</td>\n",
       "      <td>0.568883</td>\n",
       "      <td>0.215947</td>\n",
       "      <td>0.718843</td>\n",
       "      <td>...</td>\n",
       "      <td>529</td>\n",
       "      <td>911</td>\n",
       "      <td>379</td>\n",
       "      <td>930</td>\n",
       "      <td>40</td>\n",
       "      <td>477</td>\n",
       "      <td>743</td>\n",
       "      <td>196</td>\n",
       "      <td>929</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset1.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714655</td>\n",
       "      <td>0.160662</td>\n",
       "      <td>0.581990</td>\n",
       "      <td>0.714032</td>\n",
       "      <td>0.175524</td>\n",
       "      <td>...</td>\n",
       "      <td>577</td>\n",
       "      <td>914</td>\n",
       "      <td>845</td>\n",
       "      <td>40</td>\n",
       "      <td>768</td>\n",
       "      <td>863</td>\n",
       "      <td>628</td>\n",
       "      <td>734</td>\n",
       "      <td>146</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset1.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.501145</td>\n",
       "      <td>0.257456</td>\n",
       "      <td>0.479422</td>\n",
       "      <td>0.823762</td>\n",
       "      <td>0.475784</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>356</td>\n",
       "      <td>182</td>\n",
       "      <td>238</td>\n",
       "      <td>906</td>\n",
       "      <td>599</td>\n",
       "      <td>158</td>\n",
       "      <td>356</td>\n",
       "      <td>122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset1.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.844773</td>\n",
       "      <td>0.316970</td>\n",
       "      <td>0.533131</td>\n",
       "      <td>0.208293</td>\n",
       "      <td>0.984779</td>\n",
       "      <td>...</td>\n",
       "      <td>610</td>\n",
       "      <td>759</td>\n",
       "      <td>602</td>\n",
       "      <td>766</td>\n",
       "      <td>290</td>\n",
       "      <td>226</td>\n",
       "      <td>453</td>\n",
       "      <td>267</td>\n",
       "      <td>863</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset1.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148695</td>\n",
       "      <td>0.357471</td>\n",
       "      <td>0.798393</td>\n",
       "      <td>0.082284</td>\n",
       "      <td>0.933044</td>\n",
       "      <td>...</td>\n",
       "      <td>918</td>\n",
       "      <td>385</td>\n",
       "      <td>151</td>\n",
       "      <td>632</td>\n",
       "      <td>344</td>\n",
       "      <td>377</td>\n",
       "      <td>392</td>\n",
       "      <td>991</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     database_0  num_0  age_0  sex_0  diagnostic_0  pre_field1_0  \\\n",
       "0  dataset1.csv      0     39      1             7      0.818327   \n",
       "1  dataset1.csv      1     61      0             1      0.714655   \n",
       "2  dataset1.csv      2     24      1             4      0.501145   \n",
       "3  dataset1.csv      3     36      0             1      0.844773   \n",
       "4  dataset1.csv      4     41      1             0      0.148695   \n",
       "\n",
       "   pre_field2_0  pre_field3_0  pre_field4_0  pre_field5_0  ...  \\\n",
       "0      0.646789      0.568883      0.215947      0.718843  ...   \n",
       "1      0.160662      0.581990      0.714032      0.175524  ...   \n",
       "2      0.257456      0.479422      0.823762      0.475784  ...   \n",
       "3      0.316970      0.533131      0.208293      0.984779  ...   \n",
       "4      0.357471      0.798393      0.082284      0.933044  ...   \n",
       "\n",
       "   post_textfeld1_0  post_textfeld2_0  post_textfeld3_0  post_textfeld4_0  \\\n",
       "0               529               911               379               930   \n",
       "1               577               914               845                40   \n",
       "2                65               356               182               238   \n",
       "3               610               759               602               766   \n",
       "4               918               385               151               632   \n",
       "\n",
       "   post_textfeld5_0  post_textfeld6_0  post_textfeld7_0  post_textfeld8_0  \\\n",
       "0                40               477               743               196   \n",
       "1               768               863               628               734   \n",
       "2               906               599               158               356   \n",
       "3               290               226               453               267   \n",
       "4               344               377               392               991   \n",
       "\n",
       "   post_textfeld9_0  post_textfeld_total_0  \n",
       "0               929                    NaN  \n",
       "1               146                    NaN  \n",
       "2               122                    NaN  \n",
       "3               863                    NaN  \n",
       "4               102                    NaN  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 49)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# extrat all columns names\n",
    "variables_base = ['database_0','num_0']\n",
    "\n",
    "varia = set()\n",
    "for dataset in map_atributes_rev:\n",
    "    varia.update(map_atributes[dataset].keys())\n",
    "variables_base.extend(list(varia))\n",
    "\n",
    "variables_base = list(variables_base)#.sort(key=sorting_algorithm)\n",
    "\n",
    "# create empty dataframe\n",
    "df = pd.DataFrame(columns=variables_base)\n",
    "\n",
    "#metadata_aux = ['Group MCT.sav']\n",
    "\n",
    "#for indx,dataset in enumerate(metadata_aux):\n",
    "for indx,dataset in enumerate(metadata):\n",
    "\n",
    "    print(indx,\"aggregating\",dataset)\n",
    "\n",
    "    # open dataset and remove spaces in columns names\n",
    "    df_aux =open_df(os.path.join('databases',dataset))\n",
    "    \n",
    "    print(\"\\tTotal pacients\",len(df_aux),\"in\",dataset)\n",
    "    # remove rows not used in the aggreation: only MCT pacients and empty rows\n",
    "    cond = '' #mct_dict[dataset].replace(\"df\",\"df_aux\")\n",
    "    \n",
    "    #df_aux['mct_0'] = 0\n",
    "    if cond != '':\n",
    "        df_aux = df_aux[eval(cond)]\n",
    "\n",
    "\n",
    "    print(\"\\tTotal net\",len(df_aux),\"in\",dataset)\n",
    "    df_aux.rename(columns={v:v.strip() for v in df_aux.columns},inplace=True)\n",
    "\n",
    "    try:\n",
    "        # extend dataframe with new columns\n",
    "        not_df_aux_keys = [key for key in map_atributes_rev[dataset].keys() if key not in df_aux.columns]\n",
    "        \n",
    "        for col in not_df_aux_keys:\n",
    "            df_aux[col] = df_aux[col[:-2]]\n",
    "    \n",
    "        # remove columns not used in the aggregation\n",
    "        df_aux = df_aux[map_atributes_rev[dataset].keys()]\n",
    "    except Exception as error:\n",
    "        print(\"WARNING!:\",error)\n",
    "        for c in map_atributes_rev[dataset]:\n",
    "            if c not in df_aux.columns:\n",
    "                print(\"\\t\",c)\n",
    "\n",
    "    # add database and num columns\n",
    "    df_aux['database_0']=dataset\n",
    "    df_aux['num_0']=df_aux.index\n",
    "\n",
    "    try:\n",
    "        \n",
    "        #rename columns to the base name\n",
    "        df_aux.rename(columns=map_atributes_rev[dataset],inplace=True)\n",
    "\n",
    "        # add database and num columns\n",
    "        #df = pd.concat([df_aux,df],join='outer',axis=0,ignore_index=True)\n",
    "        df = (df_aux.copy() if df.empty else df.copy() if df_aux.empty else pd.concat([df_aux,df],join='outer',axis=0,ignore_index=True)) # if both DataFrames non empty\n",
    "\n",
    "    except Exception as error:\n",
    "        print(\"ERRROR:\",error)\n",
    "\n",
    "variables_base.sort(key=sorting_key)\n",
    "\n",
    "today = datetime.today()\n",
    "# Format as a string\n",
    "date_string = today.strftime('%Y-%m-%d')\n",
    "\n",
    "output = 'output'\n",
    "\n",
    "# Using DataFrame.reindex() to change columns order\n",
    "df = df.reindex(columns=variables_base)\n",
    "df.to_csv(output+os.sep+f'permepsy_{date_string}_.csv',index=False)\n",
    "\n",
    "display(df.head())\n",
    "print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
