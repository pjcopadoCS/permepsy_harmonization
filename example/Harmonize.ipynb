{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e4c390",
   "metadata": {},
   "source": [
    "# Harmonise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118f0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read complete dataset permepsy as dataframe\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "file_base = 'output'+os.sep+'permepsy_2025-02-13_.csv'\n",
    "df = pd.read_csv(file_base)\n",
    "\n",
    "\n",
    "# variables to harmonise\n",
    "#variables_to_harmonise = ['gaf.csv','Eq5d.csv','Cbq.csv','Ipsaq.csv','Rses.csv','Q-les-q-18.csv','Psyrats.csv','Slds.csv','Scip.csv','Whoqol.csv','Pbiq.csv','Dass.csv','Cdss.csv','Depression.csv','Completion.csv','QoLS.csv','SECL.csv','D15.csv','Jtc.csv','Tmt.csv','Bdi.csv','Pdi.csv','QoL.csv','Panss.csv']\n",
    "variables_to_harmonise = [\"socialdemographics\",\"variable1\",\"variable2\",\"variable3\"]\n",
    "\n",
    "# Read items of each variable\n",
    "datasets = df['database_0'].drop_duplicates().tolist()\n",
    "variables = {}\n",
    "for var in variables_to_harmonise:\n",
    "    with open('variables'+os.sep+var+\".csv\") as f:\n",
    "        lines = f.readlines()\n",
    "        variables[var] = [line.strip() for line in lines]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4dc6b",
   "metadata": {},
   "source": [
    "# Create folder called rules in each aggreation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd96e92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: harmonization/__init__.py\n",
      "File already exists: harmonization/dataset1_csv/__init__.py\n",
      "File already exists: harmonization/dataset1_csv/socialdemographics/__init__.py\n",
      "File already exists: harmonization/dataset1_csv/variable1/__init__.py\n",
      "File already exists: harmonization/dataset2_csv/__init__.py\n",
      "File already exists: harmonization/dataset2_csv/socialdemographics/__init__.py\n",
      "File already exists: harmonization/dataset2_csv/variable1/__init__.py\n",
      "File already exists: harmonization/dataset3_csv/__init__.py\n",
      "File already exists: harmonization/dataset3_csv/socialdemographics/__init__.py\n",
      "File already exists: harmonization/dataset3_csv/variable1/__init__.py\n",
      "['dataset1.csv', 'dataset2.csv', 'dataset3.csv']\n"
     ]
    }
   ],
   "source": [
    "# create folder called rules in each aggreation dataset \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def create_file(file_path,file):\n",
    "    file_name = file_path+os.sep+file\n",
    "    try:\n",
    "        with open(file_name, 'x') as f:\n",
    "            pass\n",
    "    except FileExistsError:\n",
    "        print(f'File already exists: {file_name}')\n",
    "\n",
    "def create_folder(file_path):\n",
    "    try:\n",
    "        os.makedirs(file_path, exist_ok=True)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "\n",
    "# variables to harmonise\n",
    "#variables_to_harmonise = ['gaf.csv','Eq5d.csv','Cbq.csv','Ipsaq.csv','Rses.csv','Q-les-q-18.csv','Psyrats.csv','Slds.csv','Scip.csv','Whoqol.csv','Pbiq.csv',\n",
    "#             'Dass.csv','Cdss.csv','Depression.csv','Completion.csv','QoLS.csv','SECL.csv','D15.csv','Jtc.csv','Tmt.csv','Bdi.csv','Pdi.csv','QoL.csv',\n",
    "#             'Bcis.csv','Panss.csv']\n",
    "\n",
    "variables_to_harmonise = ['socialdemographics','variable1']\n",
    "file_path = 'harmonization'\n",
    "create_folder(file_path)\n",
    "create_file(file_path,'__init__.py')\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset = dataset.replace(\" \",\"_\").replace(\".\",\"_\").lower()\n",
    "    \n",
    "    file_path += os.sep+dataset\n",
    "    create_folder(file_path)\n",
    "    create_file(file_path,'__init__.py')\n",
    "    \n",
    "    for var in variables_to_harmonise:\n",
    "        \n",
    "        var = var.replace(\".\",\"_\").lower()\n",
    "        # only if folder does not exist\n",
    "        file_path += os.sep+var\n",
    "        create_folder(file_path)\n",
    "        create_file(file_path,'__init__.py')\n",
    "        if not os.path.isfile(file_path+os.sep+'rules.py'):\n",
    "            with open(file_path+os.sep+'rules.py','w') as f:\n",
    "                f.write('def harmonize(df,dataset,var):\\n')\n",
    "                f.write('\\tpass')\n",
    "        file_path = 'harmonization'+os.sep+dataset\n",
    "        \n",
    "    \n",
    "    file_path = 'harmonization'\n",
    "    \n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00813507",
   "metadata": {},
   "source": [
    "# Go throught all datasets and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b760852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonizing socialdemographics...on dataset1.csv\n",
      "Previous dimension: (39, 3)\n",
      "Computer dataset1.csv socialdemographics  ['age', 'sex', 'diagnostic'] to []\n",
      "Final dimension: (39, 3)\n",
      "================End of harmonization=====================\n",
      "Harmonizing socialdemographics...on dataset2.csv\n",
      "Previous dimension: (51, 3)\n",
      "Computer dataset2.csv socialdemographics  ['age', 'sex', 'diagnostic'] to []\n",
      "Final dimension: (51, 3)\n",
      "================End of harmonization=====================\n",
      "Harmonizing socialdemographics...on dataset3.csv\n",
      "Previous dimension: (53, 3)\n",
      "Computer dataset3.csv socialdemographics  ['age', 'sex', 'diagnostic'] to []\n",
      "Final dimension: (53, 3)\n",
      "================End of harmonization=====================\n",
      "Harmonizing variable1...on dataset1.csv\n",
      "Previous dimension: (39, 12)\n",
      "Computer dataset1.csv variable1 pre ['field1', 'field2', 'field3', 'field4', 'field5', 'field6'] to ['field_T']\n",
      "Computer dataset1.csv variable1 post ['field1', 'field2', 'field3', 'field4', 'field5', 'field6'] to ['field_T']\n",
      "Final dimension: (39, 14)\n",
      "================End of harmonization=====================\n",
      "Harmonizing variable1...on dataset2.csv\n",
      "Previous dimension: (51, 12)\n",
      "Computer dataset2.csv variable1 pre ['field1', 'field2', 'field3', 'field4', 'field5', 'field6'] to ['field_T']\n",
      "Computer dataset2.csv variable1 post ['field1', 'field2', 'field3', 'field4', 'field5', 'field6'] to ['field_T']\n",
      "Final dimension: (51, 14)\n",
      "================End of harmonization=====================\n",
      "Harmonizing variable1...on dataset3.csv\n",
      "Previous dimension: (53, 12)\n",
      "Computer dataset3.csv variable1 pre ['field1', 'field2', 'field3', 'field4', 'field5', 'field6'] to ['field_T']\n",
      "Computer dataset3.csv variable1 post ['field1', 'field2', 'field3', 'field4', 'field5', 'field6'] to ['field_T']\n",
      "Final dimension: (53, 14)\n",
      "================End of harmonization=====================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import pandas as pd\n",
    "\n",
    "#input_file = 'harmonization'+os.sep+'input_db'+os.sep+'permepsy_2024-03-26_.csv'\n",
    "input_file = 'output'+os.sep+'permepsy_2025-02-13_.csv'\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv(input_file)\n",
    "#display (df.columns)\n",
    "\n",
    "file_path = 'harmonization'\n",
    "\n",
    "#variables_to_harmonise=['Pbiq.csv','Rses.csv','Psyrats.csv','Ipsaq.csv','CBQ.csv','Jtc.csv','Tmt.csv','Pdi.csv','Bdi.csv','Depression.csv','QoL.csv']\n",
    "#variables_to_harmonise=['Ipsaq.csv']\n",
    "#datasets = ['Leanza et al 2020 (Andreou).sav','Group MCT.sav','Database PAoS_updated.sav','UKE MCT+Psychol Database 1.sav','UKE MCT CogPack Database 2 .sav','DataBase_UV.xlsx',\n",
    "#            'Individual MCT+.sav','Rehabilitation MCT.sav','Ishikawa et al 2020_without_password_(Tanoue et al. 2021).xlsx','Tanoue et al 2021.xlsx','UKE MCT BRAT Database 3.sav','Kuokkanen et al 2014.sav',\n",
    "#            'So et al 2015.sav','Lopez-Morinigo et al 2023.sav','Balzan et al 2019.sav','Ussorio et al 2013-2016 (Roncone).sav','de Pinho et al 2021 MCT_COMPLETED PARTICIPANTS_2.sav','Favrod et al 2014.sav',\n",
    "#            'Swanson et al_tranp_ 2017.xlsx','Yildiz et al 2018.sav','Ussorio et al 2013-2016 (Roncone).sav','Fuji et al 2021_transf_(Kobayashi).xlsx','Simon Exposito et al 2019.sav']\n",
    "\n",
    "#datasets = ['UKE MCT CogPack Database 2 .sav',\n",
    "#            'UKE MCT+Psychol Database 1.sav']\n",
    "\n",
    "for var in variables_to_harmonise:\n",
    "    \n",
    "    var_name = var.replace(\".\",\"_\").lower()\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        \n",
    "        dataset_name = dataset.replace(\" \",\"_\").replace(\".\",\"_\").lower()\n",
    "          \n",
    "        file_path = 'harmonization'+'.'+dataset_name+'.'+var_name+'.rules'\n",
    "        if file_path in sys.modules:\n",
    "            print(\"Removing module from cache\")\n",
    "            del sys.modules[file_path]\n",
    "        \n",
    "        module = importlib.import_module(file_path)\n",
    "        try:\n",
    "            module.harmonize(df,dataset,var)\n",
    "            del sys.modules[file_path]\n",
    "        except Exception as e:\n",
    "            print(\"ERROR!\",dataset,var,str(e))\n",
    "            del sys.modules[file_path]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75c653",
   "metadata": {},
   "source": [
    "# Integration of previous harmonization\n",
    "##  dataset_variable ---> one variable to all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c892a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socialdemographics 3 [('after_dataset1.csv', 'output/after_dataset1.csv_socialdemographics.csv'), ('after_dataset2.csv', 'output/after_dataset2.csv_socialdemographics.csv'), ('after_dataset3.csv', 'output/after_dataset3.csv_socialdemographics.csv')]\n",
      "variable1 3 [('after_dataset1.csv', 'output/after_dataset1.csv_variable1.csv'), ('after_dataset2.csv', 'output/after_dataset2.csv_variable1.csv'), ('after_dataset3.csv', 'output/after_dataset3.csv_variable1.csv')]\n",
      "socialdemographics\n",
      "\tafter_dataset1.csv\n",
      "\tafter_dataset2.csv\n",
      "\tafter_dataset3.csv\n",
      "socialdemographics (143, 5)\n",
      "variable1\n",
      "\tafter_dataset1.csv\n",
      "\tafter_dataset2.csv\n",
      "\tafter_dataset3.csv\n",
      "variable1 (143, 16)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "folder_output = 'output'\n",
    "\n",
    "# Get all .csv files in the output folder\n",
    "files = glob.glob(folder_output + os.sep + '*.csv')\n",
    "\n",
    "# Filter files based on a condition\n",
    "filtered_files = [f for f in files if 'after' in f]\n",
    "\n",
    "variable_dataset = {k:[] for k in variables_to_harmonise}\n",
    "# Now 'filtered_files' contains all .txt files in the 'output' folder that contain 'condition' in their filename\n",
    "for file in filtered_files:\n",
    "    #print(file)\n",
    "    tokens = file.split(os.sep)[-1].split('_')\n",
    "    variable = tokens[-1].split('.')[0]\n",
    "    dataset = '_'.join(tokens[:-1])\n",
    "    \n",
    "    variable_dataset[variable].append((dataset,file))\n",
    "        \n",
    "for variable in variable_dataset:\n",
    "    print(variable,len(variable_dataset[variable]),variable_dataset[variable])\n",
    "        \n",
    "output_folder = 'output'\n",
    "\n",
    "def open_dataset(file,dataset):\n",
    "    df_new = pd.read_csv(file)\n",
    "    #try:\n",
    "        #df_new[df_new<0] = \"#N/A\"\n",
    "    #except Exception as e:\n",
    "        #print(f\"Error in {file} {dataset}\")\n",
    "    df_new['database_0']= dataset\n",
    "    df_new['num_0'] = df_new.index\n",
    "    col_0 = df_new.pop('database_0')\n",
    "    col_1 = df_new.pop('num_0')\n",
    "    df_new.insert(0, col_0.name, col_0)\n",
    "    df_new.insert(1, col_1.name, col_1)\n",
    "    return df_new\n",
    "\n",
    "for variable in variable_dataset:\n",
    "\n",
    "    df_new = pd.DataFrame()\n",
    "    print(variable)\n",
    "    for dataset,file in variable_dataset[variable]:\n",
    "        print(\"\\t\"+dataset)\n",
    "        \n",
    "        if df_new.empty:\n",
    "            df_new = open_dataset(file,dataset)\n",
    "        else:\n",
    "            df = open_dataset(file,dataset)\n",
    "            df_new = pd.concat([df_new,df],axis=0)\n",
    "        \n",
    "    print(variable,df_new.shape)\n",
    "    df_new.to_csv(output_folder+os.sep+'r_'+variable+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e1a8b",
   "metadata": {},
   "source": [
    "# Auxiliar Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03903534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorization_values(v):\n",
    "    if pd.isna(v) is None or pd.isna(v):\n",
    "        return pd.NA\n",
    "    elif isinstance(v,str):\n",
    "        return pd.NA\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186bcf3",
   "metadata": {},
   "source": [
    "# Derive variable Completation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b08f410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_0</th>\n",
       "      <th>num_0</th>\n",
       "      <th>age_0</th>\n",
       "      <th>sex_0</th>\n",
       "      <th>diagnostic_0</th>\n",
       "      <th>pre_field1_0</th>\n",
       "      <th>pre_field2_0</th>\n",
       "      <th>pre_field3_0</th>\n",
       "      <th>pre_field4_0</th>\n",
       "      <th>pre_field5_0</th>\n",
       "      <th>pre_field6_0</th>\n",
       "      <th>pre_field_T_0</th>\n",
       "      <th>post_field1_0</th>\n",
       "      <th>post_field2_0</th>\n",
       "      <th>post_field3_0</th>\n",
       "      <th>post_field4_0</th>\n",
       "      <th>post_field5_0</th>\n",
       "      <th>post_field6_0</th>\n",
       "      <th>post_field_T_0</th>\n",
       "      <th>completation_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after_dataset1.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.818327</td>\n",
       "      <td>0.646789</td>\n",
       "      <td>0.568883</td>\n",
       "      <td>0.215947</td>\n",
       "      <td>0.718843</td>\n",
       "      <td>0.346883</td>\n",
       "      <td>3.315671</td>\n",
       "      <td>0.339531</td>\n",
       "      <td>0.492845</td>\n",
       "      <td>0.956332</td>\n",
       "      <td>0.552298</td>\n",
       "      <td>0.453122</td>\n",
       "      <td>0.814832</td>\n",
       "      <td>3.608959</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>after_dataset1.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714655</td>\n",
       "      <td>0.160662</td>\n",
       "      <td>0.581990</td>\n",
       "      <td>0.714032</td>\n",
       "      <td>0.175524</td>\n",
       "      <td>0.062252</td>\n",
       "      <td>2.409116</td>\n",
       "      <td>0.763656</td>\n",
       "      <td>0.528463</td>\n",
       "      <td>0.902284</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>0.170312</td>\n",
       "      <td>0.349137</td>\n",
       "      <td>2.785675</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>after_dataset1.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.501145</td>\n",
       "      <td>0.257456</td>\n",
       "      <td>0.479422</td>\n",
       "      <td>0.823762</td>\n",
       "      <td>0.475784</td>\n",
       "      <td>0.072931</td>\n",
       "      <td>2.610500</td>\n",
       "      <td>0.288284</td>\n",
       "      <td>0.027313</td>\n",
       "      <td>0.186142</td>\n",
       "      <td>0.465648</td>\n",
       "      <td>0.869517</td>\n",
       "      <td>0.391769</td>\n",
       "      <td>2.228673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>after_dataset1.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.844773</td>\n",
       "      <td>0.316970</td>\n",
       "      <td>0.533131</td>\n",
       "      <td>0.208293</td>\n",
       "      <td>0.984779</td>\n",
       "      <td>0.878314</td>\n",
       "      <td>3.766260</td>\n",
       "      <td>0.196849</td>\n",
       "      <td>0.544306</td>\n",
       "      <td>0.179782</td>\n",
       "      <td>0.281382</td>\n",
       "      <td>0.504688</td>\n",
       "      <td>0.354778</td>\n",
       "      <td>2.061785</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>after_dataset1.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148695</td>\n",
       "      <td>0.357471</td>\n",
       "      <td>0.798393</td>\n",
       "      <td>0.082284</td>\n",
       "      <td>0.933044</td>\n",
       "      <td>0.962511</td>\n",
       "      <td>3.282397</td>\n",
       "      <td>0.231430</td>\n",
       "      <td>0.378443</td>\n",
       "      <td>0.531907</td>\n",
       "      <td>0.672035</td>\n",
       "      <td>0.040519</td>\n",
       "      <td>0.093020</td>\n",
       "      <td>1.947353</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           database_0  num_0  age_0  sex_0  diagnostic_0  pre_field1_0  \\\n",
       "1  after_dataset1.csv      0     39      1             7      0.818327   \n",
       "2  after_dataset1.csv      1     61      0             1      0.714655   \n",
       "3  after_dataset1.csv      2     24      1             4      0.501145   \n",
       "4  after_dataset1.csv      3     36      0             1      0.844773   \n",
       "5  after_dataset1.csv      4     41      1             0      0.148695   \n",
       "\n",
       "   pre_field2_0  pre_field3_0  pre_field4_0  pre_field5_0  pre_field6_0  \\\n",
       "1      0.646789      0.568883      0.215947      0.718843      0.346883   \n",
       "2      0.160662      0.581990      0.714032      0.175524      0.062252   \n",
       "3      0.257456      0.479422      0.823762      0.475784      0.072931   \n",
       "4      0.316970      0.533131      0.208293      0.984779      0.878314   \n",
       "5      0.357471      0.798393      0.082284      0.933044      0.962511   \n",
       "\n",
       "   pre_field_T_0  post_field1_0  post_field2_0  post_field3_0  post_field4_0  \\\n",
       "1       3.315671       0.339531       0.492845       0.956332       0.552298   \n",
       "2       2.409116       0.763656       0.528463       0.902284       0.071823   \n",
       "3       2.610500       0.288284       0.027313       0.186142       0.465648   \n",
       "4       3.766260       0.196849       0.544306       0.179782       0.281382   \n",
       "5       3.282397       0.231430       0.378443       0.531907       0.672035   \n",
       "\n",
       "   post_field5_0  post_field6_0  post_field_T_0  completation_0  \n",
       "1       0.453122       0.814832        3.608959             1.0  \n",
       "2       0.170312       0.349137        2.785675             1.0  \n",
       "3       0.869517       0.391769        2.228673             1.0  \n",
       "4       0.504688       0.354778        2.061785             1.0  \n",
       "5       0.040519       0.093020        1.947353             1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# create a file with name socio_demographic_data.csv replacing data by the data of today\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# adding variable completation\n",
    "def adding_completation(df):\n",
    "    \n",
    "    post_items = [it for it in df.columns if it.startswith('post')]\n",
    "    for indx,row in df.iterrows():\n",
    "        completation = not np.all(row[post_items].eq(\"N/A\"))\n",
    "        df.loc[indx,'completation_0'] = 1 if completation else 0\n",
    "\n",
    "\n",
    "base_output_files = 'output'\n",
    "\n",
    "df_base = pd.read_csv(file_base)\n",
    "df_base = df_base[['database_0','num_0']]\n",
    "df_base.rename(columns={col:col+\"_0\" for col in df_base.columns if not col.endswith(\"_0\")}, errors=\"raise\",inplace=True)\n",
    "\n",
    "'''for name in df_base.columns:\n",
    "    print(name)'''\n",
    "\n",
    "df_base['database_0'] = df_base['database_0'].str.lower().str.strip()    \n",
    "#df_base = df_base[df_base.columns[1:]]\n",
    "\n",
    "#readme_file(path,'w',f'File created on {date_string}')\n",
    "\n",
    "list_of_files_to_integrage = variables_to_harmonise\n",
    "for file in list_of_files_to_integrage:\n",
    "    \n",
    "    #readme_file(path,'a',file+'\\n')\n",
    "\n",
    "    df = pd.read_csv(base_output_files+os.sep+'r_'+file+\".csv\")    \n",
    "    df['database_0'] = df['database_0'].str.lower().str.strip()\n",
    "    \n",
    "    df_base = pd.merge(df_base,df,how='right',on=['database_0','num_0'])\n",
    "    \n",
    "\n",
    "df_base.fillna(\"NA\",inplace=True)\n",
    "df_base.index += 1\n",
    "adding_completation(df_base)\n",
    "\n",
    "# Create final file\n",
    "# Get today's date\n",
    "today = datetime.today()\n",
    "date_string = today.strftime('%Y-%m-%d')\n",
    "filename = f'harmonised_data_{date_string}.csv'\n",
    "display(df_base.head())\n",
    "df_base.to_csv('output'+os.sep+filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
