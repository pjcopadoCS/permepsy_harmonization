{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f11d7211",
   "metadata": {},
   "source": [
    "# Algorithm for datasets aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1f275",
   "metadata": {},
   "source": [
    "## Auxiliar function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Auxilar function to open files\n",
    "def open_df(file_path):\n",
    "    if \".sav\" in file_path:\n",
    "        df = pd.read_spss(file_path,convert_categoricals=False)\n",
    "    else:\n",
    "        df = pd.read_excel(file_path,engine=\"openpyxl\")\n",
    "    print(file_path)\n",
    "    print(\"Before cleaning:\",len(df))\n",
    "    df.dropna(how='all',inplace=True)\n",
    "    print(\"Afer cleaning:\",len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6caf05",
   "metadata": {},
   "source": [
    "## Read folder \\& and construct metadata dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e11be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# create dictionary meta-data from aggregation folder\n",
    "\n",
    "#os.chdir('D:\\\\MyDisk\\\\Working-PERMEPSY\\\\databases')\n",
    "os.chdir('/home/pedro/Nextcloud/Working-PERMEPSY/databases')\n",
    "\n",
    "def clean_spaces(l):\n",
    "    return [w.strip() for w in l]\n",
    "\n",
    "# Todo: read datasets names from input_file folder\n",
    "\n",
    "# read folder aggregation\n",
    "metadata = {}\n",
    "for dataset in os.listdir('aggregation'):\n",
    "    if os.path.isdir(os.path.join('aggregation',dataset)):\n",
    "        metadata[dataset] = {}\n",
    "        #print(dataset)\n",
    "        for constructo in os.listdir(os.path.join('aggregation',dataset)):\n",
    "            if constructo.endswith('.csv'):\n",
    "                constructo = constructo.replace('.csv','')\n",
    "                metadata[dataset][constructo] = {}\n",
    "                \n",
    "                # In windows\n",
    "                #df = pd.read_csv(os.path.join('aggregation',dataset,constructo+\".csv\"),encoding='ansi',sep=',')\n",
    "                # In linux\n",
    "                df = pd.read_csv(os.path.join('aggregation',dataset,constructo+\".csv\"),encoding='cp1252',sep=',')\n",
    "                df.to_excel(os.path.join('aggregation',dataset,constructo+\".xlsx\"),index=False)\n",
    "\n",
    "                # In linux\n",
    "                with open(os.path.join('aggregation',dataset,constructo+\".csv\"),'r',encoding='cp1252') as f:\n",
    "                # In windows\n",
    "                #with open(os.path.join('aggregation',dataset,constructo+\".csv\"),'r',encoding='ansi') as f:\n",
    "                    lines = f.readlines()\n",
    "                    #print(\"->\",constructo)\n",
    "                    if constructo == 'socialdemographics':\n",
    "                        keys = clean_spaces(lines[0].replace('\\n','').split(','))\n",
    "                        values = clean_spaces(lines[1].replace('\\n','').split(','))\n",
    "                        metadata[dataset][constructo] = dict(zip(keys,values))\n",
    "                    else:\n",
    "                        keys = clean_spaces(lines[0].replace('\\n','').split(','))\n",
    "                        esp = clean_spaces(lines[1].replace('\\n','').split(','))\n",
    "                        pos = clean_spaces(lines[2].replace('\\n','').split(','))\n",
    "                        metadata[dataset][constructo] = dict(zip(keys[1:],zip(esp[1:],pos[1:])))\n",
    "                        \n",
    "for dataset in metadata:\n",
    "    for constructo in metadata[dataset]:\n",
    "        print(dataset,constructo,metadata[dataset][constructo])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b2ec4",
   "metadata": {},
   "source": [
    "## Build the algorithms dictionary with columns names to matching between each dataset and the aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dataset(datset,ma,mar,k,v,i):\n",
    "    \n",
    "    ma[dataset][k+'_'+str(i)] = v\n",
    "    if v!='666' and v !='':\n",
    "        if v not in mar[dataset]:\n",
    "            mar[dataset][v] = k+'_'+str(i)\n",
    "        else:\n",
    "            count = sum(key.startswith(k+'_') for key in mar[dataset].keys())\n",
    "            mar[dataset][v+'_'+str(count)] = k+'_'+str(i)\n",
    "                \n",
    "\n",
    "df_datasets = {}\n",
    "map_atributes = {}\n",
    "map_atributes_rev = {}\n",
    "for dataset in metadata:\n",
    "    \n",
    "    map_atributes[dataset] = {}\n",
    "    map_atributes_rev[dataset] = {}\n",
    "    \n",
    "    df_datasets[dataset]= open_df(os.path.join(dataset))\n",
    "\n",
    "    for constructo in metadata[dataset]:\n",
    "        if constructo == 'socialdemographics':\n",
    "            for key,value in metadata[dataset][constructo].items():\n",
    "                #if key == 'quototal':\n",
    "                #     print(dataset,constructo,key,value)\n",
    "                #print(dataset,key,value)\n",
    "                values = value.split(';')\n",
    "                for indx,v in enumerate(values):\n",
    "                        \n",
    "                    #map_atributes[dataset][key+'_'+str(indx)] = v\n",
    "                    #if v!='666' and v !='':\n",
    "                    #    map_atributes_rev[dataset][v] = key+'_'+str(indx)\n",
    "                             \n",
    "                    insert_dataset(dataset,map_atributes,map_atributes_rev,key,v,indx)\n",
    "        else:\n",
    "            for key,value in metadata[dataset][constructo].items():\n",
    "\n",
    "                '''if key == 'qoltotal':\n",
    "                     print(dataset,constructo,key,value)'''\n",
    "                \n",
    "                values = value[0].split(';')\n",
    "                for indx,v in enumerate(values):\n",
    "\n",
    "                        #map_atributes[dataset][\"pre_\"+key+'_'+str(indx)] = v\n",
    "                        #if v!='666' and v !='':\n",
    "                        #    map_atributes_rev[dataset][v] = \"pre_\"+key+'_'+str(indx)\n",
    "                        insert_dataset(dataset,map_atributes,map_atributes_rev,\"pre_\"+key,v,indx)\n",
    "                \n",
    "                values = value[1].split(';')\n",
    "                for indx,v in enumerate(values):\n",
    "                    \n",
    "                           \n",
    "                        insert_dataset(dataset,map_atributes,map_atributes_rev,\"post_\"+key,v,indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf51bdd",
   "metadata": {},
   "source": [
    "## Auxiliar functions keeping a logical order in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Determine order of columns\n",
    "base_folder = '../databases/variables/'\n",
    "\n",
    "# Impose order of list of variables\n",
    "variables_order = [\"socialdemographics\",\"Bcis\",\"Bdi\",\"Calgary\",\"Cbq\",\"Cdss\",\"Completion\",\"D15\",\"Dass\",\"Depression\",\"Eq5d\",\"Gaf\",\"Ipsaq\",\"Jtc\",\"Panns\",\"Pbiq\",\"PDD\",\"Pdi\",\"Psyrats\",\"Q-les-q-18\",\"QoLS\",\"Rses\",\"Scip\",\"SECL\",\"Slds\",\"Tmt\",\"Whoqol\"]\n",
    "\n",
    "\n",
    "count_variables = [2]\n",
    "variables = ['database','num','mct']\n",
    "for var in variables_order:\n",
    "    with open(base_folder+var+\".csv\",\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        if lines != []:\n",
    "            aux = lines[0].strip().split(',')\n",
    "            count_variables.append(len(aux))\n",
    "            variables.extend(aux)\n",
    "\n",
    "for var in variables:\n",
    "    print(var)\n",
    "\n",
    "\n",
    "def sorting_key(c):\n",
    "\n",
    "    pre = 0\n",
    "    if c.startswith('pre_') or c.startswith('post_'):\n",
    "        \n",
    "        if c.startswith('pre_'):\n",
    "            pre = len(variables)\n",
    "        else:\n",
    "            pre = 2*len(variables)\n",
    "\n",
    "        first = c.find(\"_\")\n",
    "        last = c[::-1].find(\"_\")\n",
    "        base = c[first+1:-last-1]\n",
    "        dx = int(c[-last:])\n",
    "        \n",
    "    else:\n",
    "        tokens = c.strip().split('_')\n",
    "        base = '_'.join(tokens[:-1])\n",
    "        dx = int(tokens[-1])  \n",
    "\n",
    "    return [pre,variables.index(base),dx]\n",
    "\n",
    "\n",
    "# Building dictionary MCT\n",
    "mct_dict = {}\n",
    "\n",
    "#base_folder = 'D:\\\\MyDisk\\\\Working-PERMEPSY\\\\databases\\\\'\n",
    "base_folder = '../databases'\n",
    "\n",
    "#with open(os.path.join(base_folder,'input_db','mct_patients.csv'),\"r\",encoding='ansi') as f:\n",
    "#with open(os.path.join(base_folder,'input_db','mct_patients.csv'),\"r\",encoding='cp1252') as f:\n",
    "with open(os.path.join(base_folder,'input_db','mct_patients.csv'),\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        dataset,_,condition = line.strip().split(';')\n",
    "        mct_dict[dataset] = condition\n",
    "\n",
    "for dataset in mct_dict:\n",
    "    print(dataset,mct_dict[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a521c80",
   "metadata": {},
   "source": [
    "## Aggregating all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e70cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# extrat all columns names\n",
    "variables_base = ['database_0','num_0']\n",
    "\n",
    "varia = set()\n",
    "for dataset in map_atributes_rev:\n",
    "    varia.update(map_atributes[dataset].keys())\n",
    "variables_base.extend(list(varia))\n",
    "\n",
    "variables_base = list(variables_base)#.sort(key=sorting_algorithm)\n",
    "\n",
    "# create empty dataframe\n",
    "df = pd.DataFrame(columns=variables_base)\n",
    "\n",
    "#metadata_aux = ['Group MCT.sav']\n",
    "\n",
    "#for indx,dataset in enumerate(metadata_aux):\n",
    "for indx,dataset in enumerate(metadata):\n",
    "\n",
    "    print(indx,\"aggregating\",dataset)\n",
    "\n",
    "    # open dataset and remove spaces in columns names\n",
    "    df_aux =open_df(os.path.join(dataset))\n",
    "    \n",
    "    print(\"\\tTotal pacients\",len(df_aux),\"in\",dataset)\n",
    "    # remove rows not used in the aggreation: only MCT pacients and empty rows\n",
    "    cond = mct_dict[dataset].replace(\"df\",\"df_aux\")\n",
    "    \n",
    "    #df_aux['mct_0'] = 0\n",
    "    if cond != '':\n",
    "        df_aux = df_aux[eval(cond)]\n",
    "\n",
    "\n",
    "    print(\"\\tTotal net\",len(df_aux),\"in\",dataset)\n",
    "    df_aux.rename(columns={v:v.strip() for v in df_aux.columns},inplace=True)\n",
    "\n",
    "    try:\n",
    "        # extend dataframe with new columns\n",
    "        not_df_aux_keys = [key for key in map_atributes_rev[dataset].keys() if key not in df_aux.columns]\n",
    "        \n",
    "        for col in not_df_aux_keys:\n",
    "            df_aux[col] = df_aux[col[:-2]]\n",
    "    \n",
    "        # remove columns not used in the aggregation\n",
    "        df_aux = df_aux[map_atributes_rev[dataset].keys()]\n",
    "    except Exception as error:\n",
    "        print(\"WARNING!:\",error)\n",
    "        for c in map_atributes_rev[dataset]:\n",
    "            if c not in df_aux.columns:\n",
    "                print(\"\\t\",c)\n",
    "\n",
    "    # add database and num columns\n",
    "    df_aux['database_0']=dataset\n",
    "    df_aux['num_0']=df_aux.index\n",
    "\n",
    "    try:\n",
    "        \n",
    "        #rename columns to the base name\n",
    "        df_aux.rename(columns=map_atributes_rev[dataset],inplace=True)\n",
    "\n",
    "        # add database and num columns\n",
    "        #df = pd.concat([df_aux,df],join='outer',axis=0,ignore_index=True)\n",
    "        df = (df_aux.copy() if df.empty else df.copy() if df_aux.empty else pd.concat([df_aux,df],join='outer',axis=0,ignore_index=True)) # if both DataFrames non empty\n",
    "\n",
    "    except Exception as error:\n",
    "        print(\"ERRROR:\",error)\n",
    "\n",
    "variables_base.sort(key=sorting_key)\n",
    "\n",
    "today = datetime.today()\n",
    "# Format as a string\n",
    "date_string = today.strftime('%Y-%m-%d')\n",
    "\n",
    "output = 'harmonization'+os.sep+'input_db'\n",
    "\n",
    "# Using DataFrame.reindex() to change columns order\n",
    "df = df.reindex(columns=variables_base)\n",
    "df.to_csv(output+os.sep+f'permepsy_{date_string}_.csv',index=False)\n",
    "\n",
    "display(df.head())\n",
    "print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpermepsy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
