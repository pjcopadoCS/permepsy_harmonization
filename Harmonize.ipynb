{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e4c390",
   "metadata": {},
   "source": [
    "# Harmonise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read complete dataset permepsy as dataframe\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('permepsy.csv')\n",
    "#display (df.columns)\n",
    "\n",
    "# variables to harmonise\n",
    "variables_to_harmonise = ['gaf.csv','Eq5d.csv','Cbq.csv','Ipsaq.csv','Rses.csv','Q-les-q-18.csv','Psyrats.csv','Slds.csv','Scip.csv','Whoqol.csv','Pbiq.csv',\n",
    "             'Dass.csv','Cdss.csv','Depression.csv','Completion.csv','QoLS.csv','SECL.csv','D15.csv','Jtc.csv','Tmt.csv','Bdi.csv','Pdi.csv','QoL.csv','Panss.csv']\n",
    "\n",
    "# Read items of each variable\n",
    "datasets = df['database_0'].drop_duplicates().tolist()\n",
    "variables = {}\n",
    "for var in variables_to_harmonise:\n",
    "    with open('variables'+os.sep+var) as f:\n",
    "        lines = f.readlines()\n",
    "        variables[var] = [line.strip() for line in lines]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4dc6b",
   "metadata": {},
   "source": [
    "# Create folder called rules in each aggreation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder called rules in each aggreation dataset \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def create_file(file_path,file):\n",
    "    file_name = file_path+os.sep+file\n",
    "    try:\n",
    "        with open(file_name, 'x') as f:\n",
    "            pass\n",
    "    except FileExistsError:\n",
    "        print(f'File already exists: {file_name}')\n",
    "\n",
    "def create_folder(file_path):\n",
    "    try:\n",
    "        os.makedirs(file_path, exist_ok=True)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "\n",
    "# variables to harmonise\n",
    "variables_to_harmonise = ['gaf.csv','Eq5d.csv','Cbq.csv','Ipsaq.csv','Rses.csv','Q-les-q-18.csv','Psyrats.csv','Slds.csv','Scip.csv','Whoqol.csv','Pbiq.csv',\n",
    "             'Dass.csv','Cdss.csv','Depression.csv','Completion.csv','QoLS.csv','SECL.csv','D15.csv','Jtc.csv','Tmt.csv','Bdi.csv','Pdi.csv','QoL.csv',\n",
    "             'Bcis.csv','Panss.csv']\n",
    "\n",
    "file_path = 'harmonization'\n",
    "create_folder(file_path)\n",
    "create_file(file_path,'__init__.py')\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset = dataset.replace(\" \",\"_\").replace(\".\",\"_\").lower()\n",
    "    \n",
    "    file_path += os.sep+dataset\n",
    "    create_folder(file_path)\n",
    "    create_file(file_path,'__init__.py')\n",
    "    \n",
    "    for var in variables_to_harmonise:\n",
    "        \n",
    "        var = var.replace(\".\",\"_\").lower()\n",
    "        # only if folder does not exist\n",
    "        file_path += os.sep+var\n",
    "        create_folder(file_path)\n",
    "        create_file(file_path,'__init__.py')\n",
    "        if not os.path.isfile(file_path+os.sep+'rules.py'):\n",
    "            with open(file_path+os.sep+'rules.py','w') as f:\n",
    "                f.write('def harmonize(df,dataset,var):\\n')\n",
    "                f.write('\\tpass')\n",
    "        file_path = 'harmonization'+os.sep+dataset\n",
    "        \n",
    "    \n",
    "    file_path = 'harmonization'\n",
    "    \n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00813507",
   "metadata": {},
   "source": [
    "# Go throught all datasets and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b760852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import pandas as pd\n",
    "\n",
    "input_file = 'harmonization'+os.sep+'input_db'+os.sep+'permepsy_2024-03-26_.csv'\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv(input_file)\n",
    "#display (df.columns)\n",
    "\n",
    "file_path = 'harmonization'\n",
    "\n",
    "variables_to_harmonise=['Pbiq.csv','Rses.csv','Psyrats.csv','Ipsaq.csv','CBQ.csv','Jtc.csv','Tmt.csv','Pdi.csv','Bdi.csv','Depression.csv','QoL.csv']\n",
    "#variables_to_harmonise=['Ipsaq.csv']\n",
    "datasets = ['Leanza et al 2020 (Andreou).sav','Group MCT.sav','Database PAoS_updated.sav','UKE MCT+Psychol Database 1.sav','UKE MCT CogPack Database 2 .sav','DataBase_UV.xlsx',\n",
    "            'Individual MCT+.sav','Rehabilitation MCT.sav','Ishikawa et al 2020_without_password_(Tanoue et al. 2021).xlsx','Tanoue et al 2021.xlsx','UKE MCT BRAT Database 3.sav','Kuokkanen et al 2014.sav',\n",
    "            'So et al 2015.sav','Lopez-Morinigo et al 2023.sav','Balzan et al 2019.sav','Ussorio et al 2013-2016 (Roncone).sav','de Pinho et al 2021 MCT_COMPLETED PARTICIPANTS_2.sav','Favrod et al 2014.sav',\n",
    "            'Swanson et al_tranp_ 2017.xlsx','Yildiz et al 2018.sav','Ussorio et al 2013-2016 (Roncone).sav','Fuji et al 2021_transf_(Kobayashi).xlsx','Simon Exposito et al 2019.sav']\n",
    "\n",
    "#datasets = ['UKE MCT CogPack Database 2 .sav',\n",
    "#            'UKE MCT+Psychol Database 1.sav']\n",
    "\n",
    "for var in variables_to_harmonise:\n",
    "    \n",
    "    var_name = var.replace(\".\",\"_\").lower()\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        \n",
    "        dataset_name = dataset.replace(\" \",\"_\").replace(\".\",\"_\").lower()\n",
    "          \n",
    "        file_path = 'harmonization'+'.'+dataset_name+'.'+var_name+'.rules'\n",
    "        if file_path in sys.modules:\n",
    "            print(\"Removing module from cache\")\n",
    "            del sys.modules[file_path]\n",
    "        \n",
    "        module = importlib.import_module(file_path)\n",
    "        try:\n",
    "            module.harmonize(df,dataset,var)\n",
    "            del sys.modules[file_path]\n",
    "        except Exception as e:\n",
    "            print(\"ERROR!\",dataset,var,str(e))\n",
    "            #traceback.print_exc()\n",
    "            del sys.modules[file_path]\n",
    "            \n",
    "        #df.to_csv(\"output\"+os.sep+f\"permepsy_{dataset_name}_{var_name}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75c653",
   "metadata": {},
   "source": [
    "# Integration of previous harmonization\n",
    "##  dataset_variable ---> one variable to all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c892a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "folder_output = 'output'\n",
    "\n",
    "# Get all .csv files in the output folder\n",
    "files = glob.glob(folder_output + os.sep + '*.csv')\n",
    "\n",
    "# Filter files based on a condition\n",
    "filtered_files = [f for f in files if 'after' in f and  'after_all' not in f]\n",
    "\n",
    "variable_dataset = {}\n",
    "# Now 'filtered_files' contains all .txt files in the 'output' folder that contain 'condition' in their filename\n",
    "for file in filtered_files:\n",
    "    #print(file)\n",
    "    tokens = file.split(\"_\")[1:]\n",
    "    variable = tokens[-1].replace(\".csv\",\"\")\n",
    "    dataset = '_'.join(tokens[:-1])\n",
    "    \n",
    "    if variable not in variable_dataset:\n",
    "        variable_dataset[variable] = [(dataset,file)]\n",
    "    else:\n",
    "        variable_dataset[variable].append((dataset,file))\n",
    "        \n",
    "for variable in variable_dataset:\n",
    "    print(variable,len(variable_dataset[variable]),variable_dataset[variable])\n",
    "        \n",
    "output_folder = 'output'\n",
    "\n",
    "def open_dataset(file,dataset):\n",
    "    df_new = pd.read_csv(file)\n",
    "    #try:\n",
    "        #df_new[df_new<0] = \"#N/A\"\n",
    "    #except Exception as e:\n",
    "        #print(f\"Error in {file} {dataset}\")\n",
    "    df_new['database_0']= dataset\n",
    "    df_new['num_0'] = df_new.index\n",
    "    col_0 = df_new.pop('database_0')\n",
    "    col_1 = df_new.pop('num_0')\n",
    "    df_new.insert(0, col_0.name, col_0)\n",
    "    df_new.insert(1, col_1.name, col_1)\n",
    "    return df_new\n",
    "\n",
    "for variable in variable_dataset:\n",
    "\n",
    "    df_new = pd.DataFrame()\n",
    "    print(variable)\n",
    "    for dataset,file in variable_dataset[variable]:\n",
    "        print(\"\\t\"+dataset)\n",
    "        \n",
    "        if df_new.empty:\n",
    "            df_new = open_dataset(file,dataset)\n",
    "        else:\n",
    "            df = open_dataset(file,dataset)\n",
    "            df_new = pd.concat([df_new,df],axis=0)\n",
    "        \n",
    "    print(variable,df_new.shape)\n",
    "    df_new.to_csv(output_folder+os.sep+'r_'+variable+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e1a8b",
   "metadata": {},
   "source": [
    "# Auxiliar Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03903534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def categorization_values(v):\n",
    "    if pd.isna(v) is None or pd.isna(v):\n",
    "        return pd.NA\n",
    "    elif isinstance(v,str):\n",
    "        return pd.NA\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186bcf3",
   "metadata": {},
   "source": [
    "# Join to socio demographic variables and completation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# create a file with name socio_demographic_data.csv replacing data by the data of today\n",
    "from datetime import datetime\n",
    "\n",
    "def readme_file(path,mode,message):\n",
    "    with open(path+os.sep+'README.md',mode) as f:\n",
    "        f.write(message+'\\n')\n",
    "\n",
    "\n",
    "def adding_completation(df):\n",
    "    \n",
    "    post_items = [it for it in df.columns if it.startswith('post')]\n",
    "    for indx,row in df.iterrows():\n",
    "        completation = not np.all(row[post_items].eq(\"N/A\"))\n",
    "        df.loc[indx,'completation_0'] = 1 if completation else 0\n",
    "\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today()\n",
    "\n",
    "# Format as a string\n",
    "date_string = today.strftime('%Y-%m-%d')\n",
    "\n",
    "# Create filename\n",
    "filename = f'socio_demographic_data_{date_string}.csv'\n",
    "\n",
    "base_file = 'harmonization'+os.sep+'input_db'+os.sep+'socio.csv'\n",
    "base_output_files = 'output'\n",
    "path = '/home/pedro/Nextcloud/PERMEPSY-UPC/sociodemographic/'\n",
    "df_base = pd.read_csv(base_file,sep=\",\")\n",
    "df_base.rename(columns={col:col+\"_0\" for col in df_base.columns if not col.endswith(\"_0\")}, errors=\"raise\",inplace=True)\n",
    "\n",
    "'''for name in df_base.columns:\n",
    "    print(name)'''\n",
    "\n",
    "df_base['database_0'] = df_base['database_0'].str.lower().str.strip()    \n",
    "df_base = df_base[df_base.columns[1:]]\n",
    "\n",
    "readme_file(path,'w',f'File created on {date_string}')\n",
    "\n",
    "list_of_files_to_integrage = ['Panss.csv','bcis.csv','gaf.csv','CBQ.csv','Ipsaq.csv','Rses.csv','Jtc.csv','Tmt.csv','Psyrats.csv','Bdi.csv','Pdi.csv','Depression.csv','QoL.csv']\n",
    "for file in list_of_files_to_integrage:\n",
    "    \n",
    "    readme_file(path,'a',file+'\\n')\n",
    "\n",
    "    df = pd.read_csv(base_output_files+os.sep+'r_'+file)    \n",
    "    df['database_0'] = df['database_0'].str.lower().str.strip()\n",
    "    \n",
    "    '''for name in df_base.columns:\n",
    "        print(name)'''\n",
    "    \n",
    "    df_base = pd.merge(df_base,df,how='left',on=['database_0','num_0'],validate=\"one_to_one\")\n",
    "    \n",
    "\n",
    "df_base.fillna(\"NA\",inplace=True)\n",
    "df_base.index += 1\n",
    "adding_completation(df_base)\n",
    "df_base.to_csv('output'+os.sep+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e88448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define source file and destination directory\n",
    "source_file = filename\n",
    "source_dir = 'output'\n",
    "source_file = source_dir+os.sep+source_file\n",
    "destination_dir = '../sociodemographic/'\n",
    "\n",
    "# Use shutil to copy the file\n",
    "shutil.copy(source_file, destination_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpermepsy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
